# ğŸ”¬ Spike: RAG Pipeline with Ollama Embeddings + ECS Deployment

> **Type:** Spike / Technical Investigation  
> **Status:** Draft  
> **Created:** 2026-02-08  
> **Author:** Coral ğŸª¸

---

## Objective

Investigate and document how to:
1. Build a RAG (Retrieval-Augmented Generation) pipeline for the AI chat feature
2. Use Ollama for local embeddings during development
3. Deploy the ingestion pipeline to AWS ECS for production

---

## 1. RAG Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INGESTION (runs once / on doc update)         â”‚
â”‚                                                                  â”‚
â”‚  Your Docs    Parse & Chunk      Embedding Model      Vector DB  â”‚
â”‚  (PDF, MD) â”€â”€â–º [LangChain] â”€â”€â–º [Ollama/Voyage] â”€â”€â–º [Pinecone]   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    QUERY TIME (per visitor question)             â”‚
â”‚                                                                  â”‚
â”‚  User Question â”€â”€â–º Embed â”€â”€â–º Vector Search â”€â”€â–º Build Prompt â”€â”€â–º Claude
â”‚                                    â”‚                    â”‚
â”‚                              top K chunks         system prompt +
â”‚                                                   retrieved context
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Ollama Embeddings Integration

### 2.1 Local Setup

```bash
# Install embedding model
ollama pull nomic-embed-text      # 768 dimensions, good balance
# OR
ollama pull mxbai-embed-large     # 1024 dimensions, higher quality
```

### 2.2 LangChain.js Integration

```typescript
// lib/embeddings.ts
import { OllamaEmbeddings } from "@langchain/community/embeddings/ollama";

export const embeddings = new OllamaEmbeddings({
  model: "nomic-embed-text",
  baseUrl: process.env.OLLAMA_BASE_URL || "http://localhost:11434",
});

// Single text
const vector = await embeddings.embedQuery("What's your experience?");

// Batch
const vectors = await embeddings.embedDocuments(chunks);
```

### 2.3 Direct API (No LangChain)

```typescript
// lib/embeddings.ts
export async function embedText(text: string): Promise<number[]> {
  const response = await fetch(`${process.env.OLLAMA_BASE_URL}/api/embeddings`, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      model: "nomic-embed-text",
      prompt: text,
    }),
  });
  
  const data = await response.json();
  return data.embedding;
}
```

---

## 3. Ingestion Pipeline

### 3.1 Script Structure

```typescript
// scripts/ingest.ts
import { embedText } from "../lib/embeddings";
import { upsertToVectorDB } from "../lib/vectorStore";
import { loadDocuments, chunkText } from "../lib/documents";

async function ingest() {
  // 1. Load docs from ./documents or S3
  const docs = await loadDocuments(process.env.DOCS_PATH || "./documents");
  
  // 2. Chunk into ~500 token pieces with overlap
  const chunks = docs.flatMap((doc) => 
    chunkText(doc.content, { 
      chunkSize: 500, 
      overlap: 100,
      metadata: { source: doc.filename }
    })
  );
  
  console.log(`ğŸ“„ ${chunks.length} chunks to embed`);
  
  // 3. Embed each chunk
  for (const chunk of chunks) {
    const vector = await embedText(chunk.text);
    
    // 4. Store in vector DB
    await upsertToVectorDB({
      id: chunk.id,
      vector,
      metadata: chunk.metadata,
      text: chunk.text,
    });
    
    console.log(`âœ… Embedded: ${chunk.id}`);
  }
  
  console.log("ğŸ‰ Ingestion complete!");
}

ingest();
```

### 3.2 Document Loading

```typescript
// lib/documents.ts
import fs from "fs";
import path from "path";
import pdf from "pdf-parse";
import mammoth from "mammoth";

export async function loadDocuments(dirPath: string) {
  const files = fs.readdirSync(dirPath);
  const docs = [];
  
  for (const file of files) {
    const filePath = path.join(dirPath, file);
    const ext = path.extname(file).toLowerCase();
    
    let content: string;
    
    if (ext === ".pdf") {
      const buffer = fs.readFileSync(filePath);
      const data = await pdf(buffer);
      content = data.text;
    } else if (ext === ".docx") {
      const result = await mammoth.extractRawText({ path: filePath });
      content = result.value;
    } else if ([".md", ".txt"].includes(ext)) {
      content = fs.readFileSync(filePath, "utf-8");
    } else {
      continue; // Skip unsupported files
    }
    
    docs.push({ filename: file, content });
  }
  
  return docs;
}
```

---

## 4. ECS Deployment Options

### Option A: ECS Task with Ollama Sidecar (GPU Required)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ECS Task (GPU Instance)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Ollama         â”‚â—„â”€â”€â”€â”‚  Ingestion Script               â”‚ â”‚
â”‚  â”‚  (sidecar)      â”‚    â”‚  - Load docs from S3            â”‚ â”‚
â”‚  â”‚  nomic-embed    â”‚    â”‚  - Chunk text                   â”‚ â”‚
â”‚  â”‚  localhost:11434â”‚    â”‚  - Call Ollama for embeddings   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  - Upsert to Pinecone           â”‚ â”‚
â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Task Definition:**
```json
{
  "family": "portfolio-ingestion",
  "requiresCompatibilities": ["EC2"],
  "cpu": "4096",
  "memory": "16384",
  "containerDefinitions": [
    {
      "name": "ollama",
      "image": "ollama/ollama:latest",
      "essential": true,
      "portMappings": [{ "containerPort": 11434 }],
      "resourceRequirements": [
        { "type": "GPU", "value": "1" }
      ]
    },
    {
      "name": "ingestion",
      "image": "your-ecr-repo/ingestion:latest",
      "essential": true,
      "dependsOn": [{ "containerName": "ollama", "condition": "HEALTHY" }],
      "environment": [
        { "name": "OLLAMA_BASE_URL", "value": "http://localhost:11434" },
        { "name": "PINECONE_API_KEY", "valueFrom": "arn:aws:secretsmanager:..." }
      ]
    }
  ]
}
```

**Pros:** Self-contained, no external API costs  
**Cons:** GPU instance required (~$0.50/hr for g4dn.xlarge)

---

### Option B: ECS Task with Managed Embedding API (Recommended)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ECS Task (Fargate - No GPU!)              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Ingestion Script                                       â”‚ â”‚
â”‚  â”‚  - Load docs from S3                                    â”‚ â”‚
â”‚  â”‚  - Chunk text                                           â”‚ â”‚
â”‚  â”‚  - Call Voyage AI / OpenAI for embeddings               â”‚ â”‚
â”‚  â”‚  - Upsert to Pinecone                                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Task Definition:**
```json
{
  "family": "portfolio-ingestion",
  "requiresCompatibilities": ["FARGATE"],
  "networkMode": "awsvpc",
  "cpu": "1024",
  "memory": "2048",
  "containerDefinitions": [
    {
      "name": "ingestion",
      "image": "your-ecr-repo/ingestion:latest",
      "essential": true,
      "environment": [
        { "name": "EMBEDDING_PROVIDER", "value": "voyage" }
      ],
      "secrets": [
        { "name": "VOYAGE_API_KEY", "valueFrom": "arn:aws:secretsmanager:..." },
        { "name": "PINECONE_API_KEY", "valueFrom": "arn:aws:secretsmanager:..." }
      ]
    }
  ]
}
```

**Pros:** No GPU needed, cheaper, simpler  
**Cons:** Small API cost (~$0.0001 per 1K tokens)

---

### Option C: Hybrid (Ollama Dev / Managed Prod)

```typescript
// lib/embeddings.ts
export async function embedText(text: string): Promise<number[]> {
  if (process.env.EMBEDDING_PROVIDER === "ollama") {
    // Local development
    return embedWithOllama(text);
  } else {
    // Production - use Voyage AI
    return embedWithVoyage(text);
  }
}
```

**Best of both worlds:**
- Free local dev with Ollama
- Cheap, fast prod with managed API

---

## 5. Cost Comparison

| Approach | Ingestion Cost | Query Cost | Complexity |
|----------|---------------|------------|------------|
| Ollama on GPU ECS | ~$0.50/hr (g4dn.xlarge) | Same instance | High |
| Voyage AI (managed) | ~$0.01 total | ~$0.10/month | Low |
| OpenAI Embeddings | ~$0.02 total | ~$0.15/month | Low |
| Ollama local (dev) | Free | N/A | Dev only |

---

## 6. Recommended Production Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PRODUCTION ARCHITECTURE                                        â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ S3 Bucket   â”‚    â”‚ ECS Task    â”‚    â”‚ Pinecone            â”‚ â”‚
â”‚  â”‚ /documents  â”‚â”€â”€â”€â–ºâ”‚ Fargate     â”‚â”€â”€â”€â–ºâ”‚ Vector DB           â”‚ â”‚
â”‚  â”‚             â”‚    â”‚ (triggered) â”‚    â”‚                     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚        â”‚                   â”‚                      â”‚             â”‚
â”‚        â”‚                   â–¼                      â”‚             â”‚
â”‚        â”‚            Voyage AI API                 â”‚             â”‚
â”‚        â”‚            (embeddings)                  â”‚             â”‚
â”‚        â”‚                                          â”‚             â”‚
â”‚  EventBridge â—„â”€â”€â”€â”€ S3 Event (on upload)          â”‚             â”‚
â”‚                                                   â”‚             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚             â”‚
â”‚  â”‚ Vercel (Next.js)                    â”‚         â”‚             â”‚
â”‚  â”‚ - Website                           â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚  â”‚ - /api/chat (Claude + Pinecone)     â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7. Spike Tasks / Next Steps

- [ ] Set up Ollama locally with `nomic-embed-text`
- [ ] Create basic ingestion script that reads markdown files
- [ ] Test embedding generation locally
- [ ] Set up Pinecone free tier account
- [ ] Test end-to-end: ingest â†’ embed â†’ store â†’ retrieve
- [ ] Create Dockerfile for ingestion container
- [ ] Set up ECS task definition (Fargate)
- [ ] Create S3 bucket for document storage
- [ ] Set up EventBridge trigger for automatic ingestion
- [ ] Benchmark: Ollama vs Voyage AI (speed, quality)

---

## 8. Open Questions

1. **Vector DB choice:** Pinecone (managed) vs Supabase pgvector (self-hosted)?
2. **Embedding model:** nomic-embed-text vs mxbai-embed-large vs Voyage?
3. **Chunk size:** 500 tokens? 800? Need to test retrieval quality.
4. **Trigger mechanism:** S3 events? GitHub Actions? Manual?
5. **Cost threshold:** What's acceptable monthly spend?

---

## References

- [LangChain.js Ollama Embeddings](https://js.langchain.com/docs/integrations/text_embedding/ollama)
- [Ollama Embedding Models](https://ollama.com/library?q=embed)
- [Voyage AI Docs](https://docs.voyageai.com/)
- [Pinecone Quickstart](https://docs.pinecone.io/docs/quickstart)
- [ECS Task Definitions](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html)

---

*Spike by Coral ğŸª¸ | 2026-02-08*
